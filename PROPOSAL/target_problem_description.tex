\section{Target Problem Description}



Model extraction seeks to help computer system developers understand complex systems by finding a simplified representation that retains useful high-level information.
This high-level information helps developers validate the system's behavior or to discover instructive counter-examples that violate implicit requirements. 
Robotic designers likewise work with highly complex systems and struggle to understand the complex interrelations between components as the robotic system moves though time and space.
Although model extraction tools like Synoptic~\cite{schneider2010synoptic} are useful for examining the relationships between temporal events, they have not, to our knowledge, been applied to spatial analysis for robotics. 
We seek to explore whether existing software model extraction tools can used to examine complex spatial events in robotic systems.

\section{Background}
A system's behavior is often recorded in system logs or execution traces, that provide some facts about the behavior of the system but are sometimes overwhelmingly massive and difficult to connect to high-level facts, even though these facts might reveal critical information about the behavior of the system.
%Tools that helps extract model from computer execution traces is Synoptic.
Synoptic takes system event logs as input and extracts high-level models based on calculating invariants and coarsening and refining an abstract model.
Synoptic and other tools usually work at an abstract level of system events, that happen in a particular order.
%The problem is to explore if model extraction techniques that are usually applied to temporal events (like network program execution traces) can be applied to spatial robotic data.
Given multiple runs of a robot completing some task, is it possible to use the traces of its sensor data to construct models of spatial events common to this task?

There are several challenges in this endeavor, including mapping existing robotic system trace data to \emph{events}, so that the spatial relationships of these events can be explored by extending previous model extraction techniques. 
This will include defining an event such as \emph{UP} as a change in the z-axis of 0.5 meters, for example, so that any fluctuations in the z-axis less than 0.5 meters will not be considered.  
The events will include basic spatial movements such as \emph{UP, DOWN, LEFT, RIGHT, FRONT, BACK} 
The next challange will be mapping sets of these events to higher-level spacial movements.  
For instance, a relatively equal number of \emph{UP}s followed by \emph{DOWN}s in an alternating pattern could be defined as \emph{BOUNCING}; similarly \emph{FRONT} alternating with \emph{BACK} could be \emph{PACING}.
We hope to characterize shapes of movements, so that a chain of \emph{FRONT,FRONT,RIGHT,BACK,BACK,LEFT} will be recognized as a \emph{RECTANGLE}.
It may also be useful to look at spatial constraints within event intervals.  
For example, we may want to know that within a certain state, the robot's motion is constrained to a consistently-sized sphere, or we may see that between two state changes, the robot is always below a certain altitude, etc.

\emph{Concrete example}
Consider the following position data coming from a trace of a robot's program execution:


xpos : 0.0519  ypos : 0.1742  zpos : 1.2231

xpos : 0.0831  ypos : 0.2014  zpos : 1.7469

xpos : 0.0427  ypos : 0.3003  zpos : 2.3333

xpos : 0.5843  ypos : 0.1967  zpos : 2.4091

xpos : 1.2082  ypos : 0.1861  zpos : 2.2984

xpos : 1.1549  ypos : 0.2989  zpos : 2.3003

xpos : 1.0227  ypos : 0.2117  zpos : 2.2811

xpos : 1.0519  ypos : 0.1647  zpos : 1.7210

xpos : 1.0813  ypos : 0.1539  zpos : 1.2093

xpos : 0.5003  ypos : 0.1702  zpos : 1.2151

xpos : 0.0001  ypos : 0.3099  zpos : 1.2177

xpos : 0.0103  ypos : 0.2774  zpos : 1.2031

The running program uses this data in a local context, so that a position value may trigger thrusters to activate in response to a gust of wind, but looking at the traces, you cannot easily derive what the robot actually \emph{did} in space.

%It is difficult to make any sense of this raw data.  
It is difficult to understand the relationships between high-level events by examining this low-level data.  
Because this kind of position data is often published in a periodic stream, the higher-level events are lost in the overwhelming quantity of lower-level data.
The \emph{xpos} value might change every millisecond, but if these values only fluctuate within a small range, you want to ignore these data points.  
Creating a set of events will make meaningful spatial movements easier to handle by only considering user-defined importance.  

\emph{Why does this matter?}  

A robot run may fail, perhaps due to a program crash or an uncompleted task.  
If the failure was not directly observed, if it is not directly linked to program calls which may be observed through dynamic invariant or modeling tools (Daikon, Synoptic), and if there are no helpful error messages, then you may want to examine its spatial behavior around failure states.
Given multiple traces of failed runs, you could see that the failure is always preceded by some spatial event, or only occurs within some constrained spatial region.
Comparing the ``success" and ``failure" spatial models against one another may give you insight to how the robot reacts to its environment which existing tools do not capture.
Analyzing spatial models across runs may also help improve the efficiency of robotic tasks.  
A model may reveal unnecessary pacing back and forth when the task at hand does not require this behavior; observing this may help the developer remove the superfluous motion.

\emph{Why are existing studies, techniques, or tools insufficient?}

There are currently no tools we are aware of that can extract models of general spatial events based on trace data. 
Synoptic can extract models based on temporal properties, and Daikon can extract invariants from traces, but these properties are often independent of the spatial models we hope to build.
Both Synoptic and Daikon build their models/invariants around method calls and object values, but in a reactive environment, the execution of code may only model a part of how the robot moves through space.
Additionally, these tools cannot meaningfully handle the large amounts of noise inherent to raw positional data.
Kuipers~\etal examined how robots can build a spatial map from trace data~\cite{kuipers1988robust}, but this is one map specific to a particular environment.
Similarly, Elfes~\cite{elfes2013occupancy} demonstrated a real-time spatial representation for robot perception, but this also addresses single runs in a particular environment and does not examine the relationships between spatial events at an abstract level.
Our tool will not try to build a precise map of the space in which it moves, but instead will look at general spatial events that occur across many runs of a task.
